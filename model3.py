# -*- coding: utf-8 -*-
"""Model3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ijh3e1byRQo5vRgw0bogM1J5DKJrn8eQ
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import seaborn as sns
# %matplotlib inline 
import matplotlib.pyplot as plt
import sklearn 
import xgboost as xgb
from imblearn.over_sampling import SMOTE
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_val_score,KFold,StratifiedKFold,GridSearchCV
from sklearn.metrics import confusion_matrix, accuracy_score,f1_score,precision_score,roc_auc_score
from sklearn.neural_network import MLPClassifier
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,GradientBoostingClassifier
from sklearn.tree import DecisionTreeClassifier

"""Functions to generate our model,handle any imbalance in the dataset, run cross-validation using k-fold and stratified k_fold, using the various metrics to evaluate our model and choose the best 3 models"""

""" we will be splitting our data into training and testing, this is done so 
that we can have a data to validate that our model did not just memorize our 
train-
ing data. If we do not split our data, and then trian and test on the same data, 
we might get 
a good accuracy but we will not be sure if the model will do better with any other 
unseen data"""
def train_test_split(data,column_to_drop):
  """We will be splitting the dataset into two fold train and test"""
  #dropping the target column
  X = data.drop([column_to_drop],axis=1)
  # getting the target column
  y = data[column_to_drop]
  from sklearn.model_selection import train_test_split
  x_train,x_test,y_train,y_test = train_test_split(X,y,test_size=0.1,random_state=0)
  """getting the counts of the target, this is done to avoid training the data on
  imbalanced data which might lead to high accuracy but bad model,
  after getting the count we will cheeck if a particular class is more than 60 percent,if it is then it is not balanced"""
  y_train_counts = y_train.value_counts() #getting the counts
  print(y_train_counts)
  #getting the first counts row and second row
  y_counts_0 = y_train_counts.iloc[0]
  y_counts_1 = y_train_counts.iloc[1]
  # summing both together
  sum_both = y_counts_0 + y_counts_1
  if (y_counts_0/sum_both)*100 >=60 or (y_counts_1/sum_both)*100 >=60:
    print('train data target is not balanced, we will need to balance it with smote')
    #importing smote and applying it on our data
    from imblearn.over_sampling import SMOTE
    smote = SMOTE(random_state=0)
    x_train_balanced,y_train_balanced = smote.fit_sample(x_train,y_train)
    new_values = (x_train_balanced,x_test.values,y_train_balanced,y_test.values)
  else:
    print("train data target is balanced")
    new_values = (x_train,x_test,y_train,y_test)
  return new_values

def get_score(model,x_train,x_test,y_train,y_test,score):
  """Here we want to get the score for each prediction with different metrics, this will be used when we run our cross-validation"""
  model.fit(x_train,y_train)
  y_pred = model.predict(x_test)
  from sklearn.metrics import f1_score,accuracy_score,roc_auc_score,precision_score,recall_score
  if score == 'f1_score':
    score =f1_score(y_test,y_pred)
  elif score =='accuracy_score':
    score =accuracy_score(y_test,y_pred)
  elif score =='roc_score':
    score = roc_auc_score(y_test,y_pred)
  elif score =='precision_score':
    score = precision_score(y_test,y_pred)
  else:
    score = recall_score(y_test,y_pred)
  return score

def kfold_validate(data,column_to_drop,score,select_best_3=True):
  """This function is meant to run a kfold cross validation on the data
  with various models and picking the best 3 models based on the 
  defined scoring metrics)"""
  #setting evaluation metrics scoring metrics or short form score
  score = score
  #importing the necessary model modules
  from sklearn.model_selection import KFold
  from sklearn.linear_model import LogisticRegression
  from sklearn.neural_network import MLPClassifier
  from xgboost import XGBClassifier
  from sklearn.svm import SVC
  from sklearn.tree import DecisionTreeClassifier
  from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier,GradientBoostingClassifier
  import numpy as np
  import pandas as pd
  #using  Kfold to perform the splitting
  kfold = KFold(n_splits=5,random_state=0,shuffle=True)
  X = data.drop([column_to_drop],axis=1)
  y= data[column_to_drop]
  score_log = []
  score_mlp = []
  score_xgb = []
  score_svc = []
  score_ada = []
  score_random = []
  score_gradient = []
  score_decision = []
  model_dict_score = {}
  
  print(score_log)
  """ After splitting we will split the data index and run a cross validation on the data and then append the score to each list"""
  for train_index,test_index in kfold.split(X,y):
    x_train,x_test,y_train,y_test=X.iloc[train_index],X.iloc[test_index],y.iloc[train_index],y.iloc[test_index]
    score_log.append(get_score(LogisticRegression(),x_train,x_test,y_train,y_test,score))
    score_mlp.append(get_score(MLPClassifier(early_stopping=True,batch_size=1000),x_train,x_test,y_train,y_test,score))
    score_xgb.append(get_score(XGBClassifier(),x_train,x_test,y_train,y_test,score))
    score_svc.append(get_score(SVC(),x_train,x_test,y_train,y_test,score))
    score_ada.append(get_score(AdaBoostClassifier(),x_train,x_test,y_train,y_test,score))
    score_random.append(get_score(RandomForestClassifier(),x_train,x_test,y_train,y_test,score))
    score_gradient.append(get_score(GradientBoostingClassifier(),x_train,x_test,y_train,y_test,score))
    score_decision.append(get_score(DecisionTreeClassifier(),x_train,x_test,y_train,y_test,score))
  model_dict_score['logit_model'] = np.mean(score_log)
  model_dict_score['mlp'] = np.mean(score_mlp)
  model_dict_score['xgb'] = np.mean(score_xgb)
  model_dict_score['svc'] = np.mean(score_svc)
  model_dict_score['random_forest'] = np.mean(score_random)
  model_dict_score['decision_tree'] = np.mean(score_decision)
  #model_dict_score['randomforest'] = np.mean(score_random)
  model_dict_score['adaboost'] = np.mean(score_ada)
  model_df = pd.DataFrame(model_dict_score.items())
  model_df.columns = ['model','scores']
  if select_best_3:
    df = model_df.sort_values(by='scores',ascending=False).head(3).reset_index(drop=True)
  else:
    df = model_df.sort_values(by='scores',ascending=False).reset_index(drop=True)
  return df

def skfold_validate(data,column_to_drop,score,select_best_3=True):
  """This function is meant to run a stratifiedkfold cross validation on the data
  with various models and picking the best 3 models based on the 
  defined scoring metrics)"""
  #setting evaluation metrics scoring metrics or short form score
  score = score
  #importing the necessary model modules
  from sklearn.model_selection import KFold, StratifiedKFold
  from sklearn.linear_model import LogisticRegression
  from sklearn.neural_network import MLPClassifier
  from xgboost import XGBClassifier
  from sklearn.svm import SVC
  from sklearn.tree import DecisionTreeClassifier
  from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier,GradientBoostingClassifier
  import numpy as np
  import pandas as pd
  #using  Stratified Kfold to perform the splitting for 5 times
  skfold = StratifiedKFold(n_splits=5,random_state=0,shuffle=True)
  X = data.drop([column_to_drop],axis=1)
  y= data[column_to_drop]
  # assigning a separate list for each scores 
  score_log = []
  score_mlp = []
  score_xgb = []
  score_svc = []
  score_ada = []
  score_random = []
  score_gradient = []
  score_decision = []
  model_dict_score = {} # dictionary to store each model metrics mean scores
  
  #splitting the data using kfold and runninga forloop
  for train_index,test_index in skfold.split(X,y):
    x_train,x_test,y_train,y_test=X.iloc[train_index],X.iloc[test_index],y.iloc[train_index],y.iloc[test_index]
    score_log.append(get_score(LogisticRegression(),x_train,x_test,y_train,y_test,score))
    score_mlp.append(get_score(MLPClassifier(early_stopping=True,batch_size=1000),x_train,x_test,y_train,y_test,score))
    score_xgb.append(get_score(XGBClassifier(),x_train,x_test,y_train,y_test,score))
    score_svc.append(get_score(SVC(),x_train,x_test,y_train,y_test,score))
    score_ada.append(get_score(AdaBoostClassifier(),x_train,x_test,y_train,y_test,score))
    score_random.append(get_score(RandomForestClassifier(),x_train,x_test,y_train,y_test,score))
    score_gradient.append(get_score(GradientBoostingClassifier(),x_train,x_test,y_train,y_test,score))
    score_decision.append(get_score(DecisionTreeClassifier(),x_train,x_test,y_train,y_test,score))
  model_dict_score['logit_model'] = np.mean(score_log)
  model_dict_score['mlp'] = np.mean(score_mlp)
  model_dict_score['xgb'] = np.mean(score_xgb)
  model_dict_score['svc'] = np.mean(score_svc)
  model_dict_score['random_forest'] = np.mean(score_random)
  model_dict_score['decision_tree'] = np.mean(score_decision)
 #model_dict_score['randomforest'] = np.mean(score_random)
  model_dict_score['adaboost'] = np.mean(score_ada)
  model_df = pd.DataFrame(model_dict_score.items())
  model_df.columns = ['model','scores']
  if select_best_3:
    df = model_df.sort_values(by='scores',ascending=False).head(3).reset_index(drop=True)
  else:
    df = model_df.sort_values(by='scores',ascending=False).reset_index(drop=True)
  return df

def make_predictions(data,column_to_drop,model):
  """ From the previous function we were able to get the 3 most efficient model
  for our predictions, here we will use the models to make predictions"""
  from sklearn.linear_model import LogisticRegression
  from sklearn.neural_network import MLPClassifier
  from xgboost import XGBClassifier
  from sklearn.svm import SVC
  from sklearn.tree import DecisionTreeClassifier
  from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier,GradientBoostingClassifier
  import numpy as np
  import pandas as pd
  x_train,x_test,y_train,y_test = train_test_split(data,column_to_drop)
  if model == 'logit_model':
    clf = MLPClassifier(early_stopping=True)
    clf.fit(x_train,y_train)
    y_pred= clf.predict(x_test)
    df_xtest = pd.DataFrame(x_test,columns = ["principal component{}".format(str(i)) for i in range(1,21)])
    df_xtest['Predicted_Subscribed'] = y_pred
  elif model =='mlp':
    clf = MLPClassifier(early_stopping=True)
    clf.fit(x_train,y_train)
    y_pred = clf.predict(x_test)
    df_xtest = pd.DataFrame(x_test,columns = ["principal component{}".format(str(i)) for i in range(1,21)])
    df_xtest['Predicted_Subscribed'] = y_pred
  elif model=='xgb':
    clf = XGBClassifier(n_jobs=10)
    clf.fit(x_train,y_train)
    y_pred = clf.predict(x_test)
    df_xtest = pd.DataFrame(x_test,columns = ["principal component{}".format(str(i)) for i in range(1,21)])
    df_xtest['Predicted_Subscribed'] = y_pred
  elif model=='svc':
    clf = SVC(kernel='rbf')
    clf.fit(x_train,y_train)
    y_pred = clf.predict(x_test)
    df_xtest = pd.DataFrame(x_test,columns = ["principal component{}".format(str(i)) for i in range(1,21)])
    df_xtest['Predicted_Subscribed'] = y_pred
  elif model=='adaboost':
    clf = AdaBoostClassifier(random_state=0)
    clf.fit(x_train,y_train)
    y_pred=clf.predict(x_test)
    df_xtest = pd.DataFrame(x_test,columns = ["principal component{}".format(str(i)) for i in range(1,21)])
    df_xtest['Predicted_Subscribed'] = y_pred
  elif model == 'random_forest':
    clf = RandomForestClassifier(max_depth=3)
    clf.fit(x_train,y_train)
    y_pred =clf.predict(x_test)
    df_xtest = pd.DataFrame(x_test,columns = ["principal component{}".format(str(i)) for i in range(1,21)])
    df_xtest['Predicted_Subscribed'] = y_pred
  elif model=='decision_tree':
    clf = DecisionTreeClassifier(min_sample_leaf=2)
    clf.fit(x_train,y_train)
    y_pred = clf.predict(x_test)
    df_xtest = pd.DataFrame(x_test,columns = ["principal component{}".format(str(i)) for i in range(1,21)])
    df_xtest['Predicted_Subscribed'] = y_pred
  return df_xtest