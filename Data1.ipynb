{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Data1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN4eWuyDPJyxYzEk5W/bXpw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Idowuilekura/bank_of_portugal_predictive-model-building/blob/master/Data1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORbdgl8e8mGP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "45ec9b42-3564-4d21-8596-9f754b1f8850"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "%matplotlib inline \n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn \n",
        "import xgboost\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score,KFold,StratifiedKFold\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score,f1_score,precision_score,roc_auc_score\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIfvObR6Z7iE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def transform_encode(data):\n",
        "  \"\"\" removing outlier to do these we will use LocalOutlierfactor, any value tha\n",
        "  t is less than one will be an outlier,the purpose of removing outliers is to prevent the model from \n",
        "  taking too long to load and misledding the model\"\"\"\n",
        "  from scipy import stats\n",
        "  from sklearn.preprocessing import StandardScaler, LabelEncoder,OneHotEncoder \n",
        "  from sklearn.neighbors import LocalOutlierFactor\n",
        "  \"\"\"duration was dropped because it has correlation with the target variable\n",
        "  if duration is 0 then the customer might not subscribed, also this was done \n",
        "  so that it will not impact our outlier removal since it is not needed in training\n",
        "  \"\"\"\n",
        "  data_1 = data.drop(['duration','y'],axis=1)\n",
        "  numerical_df = data_1.select_dtypes(include=['int','float'])#selecting float and int columns\n",
        "  list_numerical_df_columns = list(numerical_df.columns)\n",
        "  \"\"\"The localoutlierfactor is another model to detect outliers,\n",
        "  any value that is less than 1 is considered an outlier since it dosen'\n",
        "  follow the uniform distribution\"\"\"\n",
        "  lof = LocalOutlierFactor()\n",
        "  yhat = lof.fit_predict(numerical_df) #fitting the localoutlier factor model\n",
        "  mask = yhat !=-1\n",
        "  data = data.loc[mask,:]\n",
        "  data_1 = data_1.loc[mask,:] #filtering out rows that are not outliers\n",
        "  for col in list_numerical_df_columns:\n",
        "    data_1[col] = StandardScaler().fit_transform(data_1[[col]]) #scaling the values so it can be on the same range\n",
        "  cat_df = data_1.select_dtypes(include=['object'])\n",
        "  cat_dumm = pd.get_dummies(cat_df) #converting the categorical data to 1 or 0\n",
        "  \"\"\"dropping the categorical columns becaue we have encoded and the old columns\n",
        "   are not needed\"\"\"\n",
        "  df = data_1.drop(list(cat_df.columns),axis=1)\n",
        "  \"\"\"concatenating the dataframe with the encoded categorical columns since we \n",
        "  had dropped the columns earlier\"\"\"\n",
        "  df = pd.concat([df,cat_dumm],axis=1)\n",
        "  #encoding the target variable y and renaming it to subscribed and joing it back \n",
        "  df['Subscribed'] = LabelEncoder().fit_transform(data['y'])\n",
        "  return df\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yo_Ioz7TLTzE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def reduce_dimension(data,reduction_model):\n",
        "  \"\"\"since our colummns are many, we need to reduce the computational time by \n",
        "  reducing the numbers of columns and still retaining useful columns, we will be using \n",
        "  principal component ananlysis,t-distributed stochastic neighbor and auto-encoders\"\"\"\n",
        "  data_1 = transform_encode(data)\n",
        "  data = data_1.drop(['Subscribed'],axis=1)\n",
        "  \"\"\" importing necessary libraries\"\"\"\n",
        "  from sklearn.decomposition import PCA \n",
        "  from sklearn.manifold import TSNE\n",
        "  from keras.models import Model \n",
        "  from keras.layers import Input,Dense\n",
        "  from keras import regularizers\n",
        "  encoding_dim= 20\n",
        "  if reduction_model == 'pca':\n",
        "    pca = PCA(n_components=20) #components to reduce the columns to\n",
        "    \"\"\" to justify why we choose 20 components from the plot we could see that \n",
        "    best components is 20 because that is where the lines starts to get constant\"\"\"\n",
        "    pca_2 = PCA().fit(data.values)\n",
        "    plt.plot(np.cumsum(pca_2.explained_variance_ratio_))\n",
        "    plt.xlabel('number of components')\n",
        "    plt.ylabel('cummulative explained variance')\n",
        "    reduced_df = pd.DataFrame(pca.fit_transform(data.values),columns = [\"principal component{}\".format(str(i)) for i in range(1,21)])\n",
        "  elif reduction_model=='tsne':\n",
        "    \"\"\" tsne maximum components is 2 hence we will go with it\"\"\"\n",
        "    tsne = TSNE(n_components=2,n_iter=300)\n",
        "    reduced_df = pd.DataFrame(tsne.fit_transform(data),columns= [\"tsne component{}\".format(str(i)) for i in range(1,3)])\n",
        "  else:\n",
        "    # fixed dimensions\n",
        "    input_dim = data.shape[1]\n",
        "    encoding_dim = 20\n",
        "    \"\"\" Number of neurons in each layer [data_columns which is input_dim,30 for \n",
        "    the first hidden layer,30 for the second hidden layer and 20 for our desired \n",
        "    output layer which is encoding dimension and it is 20]. Also for eah encoded layer we will be passing\n",
        "    the output fromone layer to the other, hence the need to make one layer input connected to the next layer\n",
        "    and our activation function will be tanh\"\"\"\n",
        "    input_layer = Input(shape=(input_dim,))\n",
        "    encoded_layer_1 = Dense(\n",
        "        40,activation='tanh',activity_regularizer=regularizers.l1(10e-5))(input_layer)\n",
        "    encoded_layer_2 = Dense(30,activation='tanh')(encoded_layer_1)\n",
        "    encoder_layer_3 = Dense(encoding_dim,activation='tanh')(encoded_layer_2)\n",
        "    # create encoder model\n",
        "    encoder = Model(inputs=input_layer,outputs=encoder_layer_3)\n",
        "    reduced_df= pd.DataFrame(encoder.predict(data)\n",
        "    )\n",
        "  print(reduced_df.shape)\n",
        "  print(data_1[['Subscribed']].shape)\n",
        "  reduced_df['Subscribed']=data_1['Subscribed'].values\n",
        "  return reduced_df\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qf7U5ueuwLkG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}