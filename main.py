# -*- coding: utf-8 -*-
"""main.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13hhk4LEMC59jqmH4h0EmQPv3PTfJAq_M
"""

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import seaborn as sns
# %matplotlib inline 
import matplotlib.pyplot as plt
import sklearn 
import xgboost as xgb
from imblearn.over_sampling import SMOTE
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_val_score,KFold,StratifiedKFold,GridSearchCV
from sklearn.metrics import confusion_matrix, accuracy_score,f1_score,precision_score,roc_auc_score
from sklearn.neural_network import MLPClassifier
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,GradientBoostingClassifier
from sklearn.tree import DecisionTreeClassifier

dataset = pd.read_csv("/content/drive/My Drive/Data science resources/bank-additional/bank-additional/bank-additional-full.csv",sep=";")

#!cp "drive/My Drive/mylib.py"

!cp "drive/My Drive/Data science resources/model.py"

import sys
sys.path.append('/content/drive/My Drive/Data science resources/model.py')

from google.colab import files 
files.upload()

import data1

from data1 import transform_encode,reduce_dimension

from model3 import kfold_validate,train_test_split,get_score,skfold_validate,make_predictions

""" here the reduced_dataset function was imported using principal component 
analysis to reduces the datset to 20 columns, the graph below is to show the 
motive of choosing the pca model as 20, here we could see that 20 was choosen
because as from 20 the graph became almost constant. In this function, the fuction took the 
dataset, removed outliers, encoded categorical values using pd.get_dummies, labelencoded 
the target value and scaled numerical dataset using standardScaler and reduced the dataset to 20"""
reduced_dataset = reduce_dimension(dataset,'pca')

reduced_dataset.head()
""" checking the values count of the target variable shows that the values are 
not balanced we will use one of the modules to balance this"""
reduced_dataset.Subscribed.value_counts()

"""this step is to compare kfold cross validation with stratified kfold cross validation, 
the metrics that will be used is the precision score,to comapre both cross validation
and choose the best 3 models"""
kfold_validate(reduced_dataset,'Subscribed','accuracy_score',False)

skfold_validate(reduced_dataset,'Subscribed','accuracy_score',False)

""" from the above we could see that stratified cross validation gave more 
accuracy and the three best models are mlp,logit_model and svc for the kfold, while for skfold_validate the best models are logitistic_regression,support vector classfier and extreme gradient booting(xgb)"""

""" function to split the data into x_train,x_test, balance the data with smote and make predictions 
with the defined model and join the predicted subscribe column to the x_test"""
make_predictions(reduced_dataset,'Subscribed','logit_model')